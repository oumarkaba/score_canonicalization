{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import kornia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18 = torchvision.models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.MNIST(root='_data', train=True, transform=torchvision.transforms.ToTensor(), download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='_data', train=False, transform=torchvision.transforms.ToTensor(), download=True)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18.fc = nn.Linear(512, 10)\n",
    "resnet18.cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(resnet18.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(5):\n",
    "    resnet18.train()\n",
    "    for i, (x, y) in enumerate(train_loader):\n",
    "        x = x.cuda().repeat(1, 3, 1, 1)\n",
    "        y = y.cuda()\n",
    "\n",
    "        y_pred = resnet18(x)\n",
    "        loss = nn.functional.cross_entropy(y_pred, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f'epoch {epoch}, iter {i}, loss {loss.item()}')\n",
    "\n",
    "    resnet18.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for x, y in test_loader:\n",
    "        x = x.cuda().repeat(1, 3, 1, 1)\n",
    "        y = y.cuda()\n",
    "\n",
    "        y_pred = resnet18(x)\n",
    "        _, y_pred = y_pred.max(dim=1)\n",
    "        correct += (y_pred == y).sum().item()\n",
    "        total += y.size(0)\n",
    "\n",
    "    print(f'epoch {epoch}, accuracy {correct / total}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/jw9730/lps/src/symmetry/groups/SO.py\n",
    "def samples_from_haar_distribution(d, bsize, device, dtype) -> torch.Tensor:\n",
    "    \"\"\"Random orthogonal matrices with determinant 1 drawn from SO(d) Haar distribution\n",
    "    Adopted from scipy.stats.special_ortho_group, which implements the algorithm described in\n",
    "    Mezzadri, How to generate random matrices from the classical compact groups (2006)\n",
    "    \"\"\"\n",
    "    # H represents a (dim, dim) matrix, while D represents the diagonal of\n",
    "    # a (dim, dim) diagonal matrix. The algorithm that follows is\n",
    "    # broadcasted on the leading shape in `size` to vectorize along\n",
    "    # samples.\n",
    "    H = torch.empty(bsize, d, d, device=device, dtype=dtype)\n",
    "    H[..., :, :] = torch.eye(d, device=device, dtype=dtype)\n",
    "    D = torch.empty(bsize, d, device=device, dtype=dtype).fill_(float('inf'))\n",
    "    for n in range(d-1):\n",
    "        # x is a vector with length dim-n, xrow and xcol are views of it as\n",
    "        # a row vector and column vector respectively. It's important they\n",
    "        # are views and not copies because we are going to modify x\n",
    "        # in-place.\n",
    "        x = torch.randn(bsize, d-n, device=device, dtype=dtype)\n",
    "        xrow = x[..., None, :]\n",
    "        xcol = x[..., :, None]\n",
    "\n",
    "        # This is the squared norm of x, without vectorization it would be\n",
    "        # dot(x, x), to have proper broadcasting we use matmul and squeeze\n",
    "        # out (convert to scalar) the resulting 1x1 matrix\n",
    "        norm2 = torch.matmul(xrow, xcol).squeeze((-2, -1))\n",
    "\n",
    "        x0 = x[..., 0].clone()\n",
    "        D[..., n] = torch.where(x0 != 0, torch.sign(x0), 1)\n",
    "        x[..., 0] += D[..., n] * torch.sqrt(norm2)\n",
    "\n",
    "        # In renormalizing x we have to append an additional axis with\n",
    "        # [..., None] to broadcast the scalar against the vector x\n",
    "        x /= torch.sqrt((norm2 - x0**2 + x[..., 0]**2) / 2.)[..., None]\n",
    "\n",
    "        # Householder transformation, without vectorization the RHS can be\n",
    "        # written as outer(H @ x, x) (apart from the slicing)\n",
    "        H[..., :, n:] -= torch.matmul(H[..., :, n:], xcol) * xrow\n",
    "\n",
    "    D[..., -1] = (-1)**(d-1) * D[..., :-1].prod(dim=-1)\n",
    "\n",
    "    # Without vectorization this could be written as H = diag(D) @ H,\n",
    "    # left-multiplication by a diagonal matrix amounts to multiplying each\n",
    "    # row of H by an element of the diagonal, so we add a dummy axis for\n",
    "    # the column index\n",
    "    H *= D[..., :, None]\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_energy(x, y):\n",
    "    y_pred = resnet18(x)\n",
    "    loss = nn.functional.cross_entropy(y_pred, y)\n",
    "    return loss\n",
    "\n",
    "def so2_action(g, x):\n",
    "    alpha = g[:, 0, 0]\n",
    "    beta = g[:, 0, 1]\n",
    "    cx = cy = 28/2\n",
    "    affine_part = torch.stack([(1 - alpha) * cx - beta * cy, beta * cx + (1 - alpha) * cy], dim=1)\n",
    "    affine_matrices = torch.cat([g, affine_part.unsqueeze(-1)], dim=-1)\n",
    "    return kornia.geometry.affine(x, affine_matrices)\n",
    "\n",
    "def so2_lie_algebra_element(alpha):\n",
    "    bsize = alpha.size(0)\n",
    "    A = torch.zeros(bsize, 2, 2, device=alpha.device)\n",
    "    A[:, 0, 1] = -alpha[:, 0]\n",
    "    A[:, 1, 0] = alpha[:, 0]\n",
    "    return A\n",
    "\n",
    "def so2_retraction(A):\n",
    "    return torch.linalg.matrix_exp(A)\n",
    "\n",
    "def so2_lie_algebra_gradient(alpha, x, y):\n",
    "    alpha = alpha.clone().detach()\n",
    "    alpha.requires_grad_(True)\n",
    "    A = so2_lie_algebra_element(alpha)\n",
    "    g = so2_retraction(A)\n",
    "    gx = so2_action(g, x)\n",
    "    energy = get_energy(gx, y)\n",
    "    grad, = torch.autograd.grad(energy, alpha, only_inputs=True)\n",
    "    return grad\n",
    "\n",
    "def annealed_langevin_dynamics(alpha, x, y, L, T, eps, sigma_1, decay_rate, grad_scale):\n",
    "    alpha_list = [alpha]\n",
    "    sigma_list = [sigma_1 * decay_rate ** i for i in range(L)]\n",
    "    step_sizes = [eps * (sigma_list[i] / sigma_list[L - 1]) ** 2 for i in range(L)]\n",
    "    for i in range(L):\n",
    "        for _ in range(T):\n",
    "            alpha = alpha_list[-1]\n",
    "            grad = so2_lie_algebra_gradient(alpha, x, y)\n",
    "            alpha = alpha - (step_sizes[i] / 2) * grad_scale * grad + math.sqrt(step_sizes[i]) * torch.randn_like(alpha)\n",
    "            alpha_list.append(alpha)\n",
    "    return alpha_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18.eval()\n",
    "\n",
    "x, y = next(iter(test_loader))\n",
    "\n",
    "# x = x.cuda().repeat(1, 3, 1, 1)\n",
    "# y = y.cuda()\n",
    "\n",
    "x = x[0:1].cuda().repeat(x.size(0), 3, 1, 1)\n",
    "y = y[0:1].cuda().repeat(x.size(0))\n",
    "\n",
    "random_g = samples_from_haar_distribution(2, x.size(0), x.device, x.dtype)\n",
    "x0 = so2_action(random_g, x)\n",
    "\n",
    "# following https://arxiv.org/pdf/1907.05600\n",
    "L = 10\n",
    "T = 100\n",
    "sigma_1 = 1\n",
    "decay_rate = 10 ** (math.log10(0.01) / 10)\n",
    "eps = 2e-5\n",
    "\n",
    "grad_scale = 100\n",
    "\n",
    "alpha = torch.randn(x0.size(0), 1).cuda()\n",
    "alpha_list = annealed_langevin_dynamics(alpha, x0, y, L, T, eps, sigma_1, decay_rate, grad_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_alpha = alpha_list[-1]\n",
    "final_A = so2_lie_algebra_element(final_alpha)\n",
    "final_g = so2_retraction(final_A)\n",
    "final_x = so2_action(final_g, x0)\n",
    "\n",
    "def accuracy(x, y):\n",
    "    y_pred = resnet18(x)\n",
    "    _, y_pred = y_pred.max(dim=1)\n",
    "    return (y_pred == y).float().mean().item()\n",
    "\n",
    "y_pred = resnet18(x)\n",
    "y0_pred = resnet18(x0)\n",
    "final_y_pred = resnet18(final_x)\n",
    "\n",
    "print(f'accuracy(x) = {accuracy(x, y)}')\n",
    "print(f'accuracy(x0) = {accuracy(x0, y)}')\n",
    "print(f'accuracy(final_x) = {accuracy(final_x, y)}')\n",
    "\n",
    "# visualize first 20 images\n",
    "x_numpy = x.cpu().detach().numpy()\n",
    "x0_numpy = x0.cpu().detach().numpy()\n",
    "final_x_numpy = final_x.cpu().detach().numpy()\n",
    "\n",
    "fig, axs = plt.subplots(3, 20, figsize=(20, 6))\n",
    "for i in range(20):\n",
    "    axs[0, i].imshow(x_numpy[-i, 0], cmap='gray')\n",
    "    axs[1, i].imshow(x0_numpy[-i, 0], cmap='gray')\n",
    "    axs[2, i].imshow(final_x_numpy[-i, 0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallUNet(nn.Module):\n",
    "    def __init__(self, in_ch=3, out_ch=3, mid_ch=32):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_ch, mid_ch, 5, padding=2)\n",
    "        self.conv2 = nn.Conv2d(mid_ch, mid_ch, 5, padding=2)\n",
    "        self.down  = nn.Conv2d(mid_ch, mid_ch*2, 3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(mid_ch*2, mid_ch*2, 3, padding=1)\n",
    "        self.up    = nn.ConvTranspose2d(mid_ch*2, mid_ch, 2, stride=2)\n",
    "        self.conv4 = nn.Conv2d(mid_ch*2, mid_ch, 5, padding=2)\n",
    "        self.conv5 = nn.Conv2d(mid_ch, out_ch, 5, padding=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B,C,H,W)\n",
    "        returns: (B,C,H,W)\n",
    "        \"\"\"\n",
    "        d1 = torch.relu(self.conv1(x))    # (B, mid_ch, H, W)\n",
    "        d1 = torch.relu(self.conv2(d1))   # (B, mid_ch, H, W)\n",
    "        d2 = torch.relu(self.down(d1))    # (B, mid_ch*2, H/2, W/2)\n",
    "        d2 = torch.relu(self.conv3(d2))   # (B, mid_ch*2, H/2, W/2)\n",
    "\n",
    "        u1 = self.up(d2)                  # (B, mid_ch, H, W)\n",
    "        cat1 = torch.cat([u1, d1], dim=1) # (B, mid_ch+mid_ch, H, W)\n",
    "        u1 = torch.relu(self.conv4(cat1)) # (B, mid_ch, H, W)\n",
    "        out = self.conv5(u1)              # (B, out_ch, H, W)\n",
    "        return out\n",
    "\n",
    "unet = SmallUNet().cuda()\n",
    "unet.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_energy_unet(gx):\n",
    "    inner = (gx * unet(gx.detach())).sum()\n",
    "    return inner\n",
    "\n",
    "def so2_lie_algebra_gradient_unet(alpha, x, create_graph):\n",
    "    alpha = alpha.clone().detach()\n",
    "    alpha.requires_grad_(True)\n",
    "    A = so2_lie_algebra_element(alpha)\n",
    "    g = so2_retraction(A)\n",
    "    gx = so2_action(g, x)\n",
    "    energy = get_energy_unet(gx)\n",
    "    grad, = torch.autograd.grad(energy, alpha, create_graph=create_graph)\n",
    "    return grad\n",
    "\n",
    "def annealed_langevin_dynamics_unet(alpha, x, L, T, eps, sigma_1, decay_rate, grad_scale):\n",
    "    alpha_list = [alpha]\n",
    "    sigma_list = [sigma_1 * decay_rate ** i for i in range(L)]\n",
    "    step_sizes = [eps * (sigma_list[i] / sigma_list[L - 1]) ** 2 for i in range(L)]\n",
    "    for i in range(L):\n",
    "        for _ in range(T):\n",
    "            alpha = alpha_list[-1]\n",
    "            grad = so2_lie_algebra_gradient_unet(alpha, x, create_graph=False)\n",
    "            alpha = alpha - (step_sizes[i] / 2) * grad_scale * grad + math.sqrt(step_sizes[i]) * torch.randn_like(alpha)\n",
    "            alpha_list.append(alpha)\n",
    "    return alpha_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18.eval()\n",
    "unet.eval()\n",
    "\n",
    "x, y = next(iter(test_loader))\n",
    "\n",
    "x = x[0:1].cuda().repeat(x.size(0), 3, 1, 1)\n",
    "y = y[0:1].cuda().repeat(x.size(0))\n",
    "\n",
    "random_g = samples_from_haar_distribution(2, x.size(0), x.device, x.dtype)\n",
    "x0 = so2_action(random_g, x)\n",
    "\n",
    "# following https://arxiv.org/pdf/1907.05600\n",
    "L = 10\n",
    "T = 100\n",
    "sigma_1 = 1\n",
    "decay_rate = 10 ** (math.log10(0.01) / 10)\n",
    "eps = 2e-5\n",
    "\n",
    "grad_scale = 100\n",
    "\n",
    "alpha = torch.randn(x0.size(0), 1).cuda()\n",
    "alpha_list = annealed_langevin_dynamics_unet(alpha, x0, L, T, eps, sigma_1, decay_rate, grad_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_alpha = alpha_list[-1]\n",
    "final_A = so2_lie_algebra_element(final_alpha)\n",
    "final_g = so2_retraction(final_A)\n",
    "final_x = so2_action(final_g, x0)\n",
    "\n",
    "def accuracy(x, y):\n",
    "    y_pred = resnet18(x)\n",
    "    _, y_pred = y_pred.max(dim=1)\n",
    "    return (y_pred == y).float().mean().item()\n",
    "\n",
    "y_pred = resnet18(x)\n",
    "y0_pred = resnet18(x0)\n",
    "final_y_pred = resnet18(final_x)\n",
    "\n",
    "print(f'accuracy(x) = {accuracy(x, y)}')\n",
    "print(f'accuracy(x0) = {accuracy(x0, y)}')\n",
    "print(f'accuracy(final_x) = {accuracy(final_x, y)}')\n",
    "\n",
    "# visualize first 20 images\n",
    "x_numpy = x.cpu().detach().numpy()\n",
    "x0_numpy = x0.cpu().detach().numpy()\n",
    "final_x_numpy = final_x.cpu().detach().numpy()\n",
    "\n",
    "fig, axs = plt.subplots(3, 20, figsize=(20, 6))\n",
    "for i in range(20):\n",
    "    axs[0, i].imshow(x_numpy[-i, 0], cmap='gray')\n",
    "    axs[1, i].imshow(x0_numpy[-i, 0], cmap='gray')\n",
    "    axs[2, i].imshow(final_x_numpy[-i, 0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18.eval()\n",
    "for p in resnet18.parameters():\n",
    "    p.requires_grad_(False)\n",
    "\n",
    "optimizer = torch.optim.Adam(unet.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(10):\n",
    "    unet.train()\n",
    "    for i, (x, y) in enumerate(train_loader):\n",
    "        x = x.cuda().repeat(1, 3, 1, 1).clone().detach()\n",
    "        y = y.cuda()\n",
    "\n",
    "        g = samples_from_haar_distribution(2, x.size(0), x.device, x.dtype)\n",
    "        gx = so2_action(g, x).clone().detach()\n",
    "        alpha = torch.randn(x.size(0), 1).cuda()\n",
    "        grad_target = so2_lie_algebra_gradient(alpha, gx, y).clone().detach()\n",
    "        grad_pred = so2_lie_algebra_gradient_unet(alpha, gx, create_graph=True)\n",
    "        loss = nn.functional.mse_loss(grad_pred, grad_target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f'epoch {epoch}, iter {i}, loss {loss.item()}')\n",
    "\n",
    "    unet.eval()\n",
    "    test_loss = 0\n",
    "    total = 0\n",
    "    for x, y in test_loader:\n",
    "        x = x.cuda().repeat(1, 3, 1, 1)\n",
    "        y = y.cuda()\n",
    "\n",
    "        g = samples_from_haar_distribution(2, x.size(0), x.device, x.dtype)\n",
    "        gx = so2_action(g, x)\n",
    "        alpha = torch.randn(x.size(0), 1).cuda()\n",
    "        grad_target = so2_lie_algebra_gradient(alpha, gx, y)\n",
    "        grad_pred = so2_lie_algebra_gradient_unet(alpha, gx)\n",
    "\n",
    "        test_loss += nn.functional.mse_loss(grad_pred, grad_target).item()\n",
    "        total += y.size(0)\n",
    "\n",
    "    print(f'epoch {epoch}, test loss {test_loss / total}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
